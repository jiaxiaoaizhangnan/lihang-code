{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第5章 决策树\n",
    "顾名思义,\"决策树\"就是按照建立分支来进行决策的方法,即根据啥(往往是根据某个属性的某个值作为指标，比如“年龄大于20岁的”或“身高大于1米5的”)来区分不同类别的数据,也是一种监督式机器学习算法,从编程角度来看,可以看作是一系列的\"if... then... else...\"的集合,这个模型非常直观,也即可读性非常好,决策树的学习通常包含三个步骤:**特征选择**,**决策树生成**,**决策树的剪枝**.  \n",
    "给定数据集\n",
    ">$$D = \\{ (x_1,y_1),(x_2,y_2),...,(x_N,y_N)\\}$$\n",
    "其中,$x_i=(x_i^{(1)},x_i^{(2)},...,x_i^{(n)})^T$为输入实例(特征向量),$n$为特征的个数,$y_i \\in \\{1,2,...,K\\}$为类别标记,$i = 1,2,...,N$,$N$为样本容量\n",
    "\n",
    "目标\n",
    ">构建决策树,使之能改对大量的实例进行正确分类（而为什么要引入决策树的剪枝呢？是要让这颗树尽可能地好，那什么叫好呢？就是用尽可能少的分叉来高效地区分更多的数据）  \n",
    "(本质是构建一组规则,这组规则可以对每个特征进行划分,但是又不能划分太好,不然容易过拟合,泛化能力就会变差)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1数据集\n",
    "\n",
    "首先建立课本上的数据集合本身  \n",
    ">年龄定义\"青年|中年|老年\"分别为\"1|2|3\",  \n",
    "是否有工作,定义\"是|否\"为\"1|2\",  \n",
    "有自己的房子,定义\"是|否\"为\"1|2\",  \n",
    "信贷情况\"一般|好|非常好\"为\"1|2|3\",  \n",
    "类别\"是|否\"为\"1|2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.asarray([[1,2,2,1],[1,2,2,2],[1,1,2,2],[1,1,1,1],[1,2,2,1],\n",
    "               [2,2,2,1],[2,2,2,2],[2,1,1,2],[2,2,1,3],[2,2,1,3],\n",
    "               [3,2,1,3],[3,2,1,2],[3,1,2,2],[3,1,2,3],[3,2,2,1]])\n",
    "y = np.asarray([2,2,1,1,2,2,2,1,1,1,1,1,1,1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 此时某些用心的读者也许还会发现,给了这些数据之后,实际可以尝试性地按照x的某一列属性作为指标,给定阈值,阈值之上的为某个列别标签,阈值之下的为另一个类别标签,但是问题来啦:选择哪个属性最好呢?\n",
    "这里不得不说到\"信息增益\"的问题了,这个东西可以指导模型选择最合适的属性对原始数据进行第一阶的划分.\n",
    "\n",
    ">这里需要对“信息增益”进行一些讲解，信息增益可以这么理解：如果对一件事进行描述，并且需要10次才能描述完整，那么实际每次描述的时候，描述所涵盖的信息量是不相同的，信息增益，就可以理解为一次描述过程的量。对于这里的决策树来说，就是每次if和else所能带有的信息。实际的定义和形式化描述如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5.2信息熵和信息增益\n",
    "### 5.2.1信息熵，经验熵，相对熵\n",
    "（1）＂**熵**＂的定义：$$H(X) = -\\sum_{i=1}^nP_i\\log{P_i}$$\n",
    ">观察上式可以得知，熵只和数据分布有关，所以，$H(X)$可以直接写作$H(p)$，且从定义可以验证\n",
    "$$0 \\leq H(p) \\leq \\log n$$\n",
    ">（关于＂熵＂为什么要这么定义，这里给一下思路提示：比如，如果要定义一个三位数，实际只用３个字符存储空间就可以了，这也就是为什么取log的原因了，实际表示的是描述某个对象所需要的基本元素的个数，更详细的关于信息论的定义可以自行学习香浓的那一套信息理论，这里我也不懂很多，只给点提示，另外，这里结合课本给出求解信息熵的代码）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter #计数插件\n",
    "def cal_entropy(x,i):\n",
    "    \"\"\"\n",
    "    计算熵值\n",
    "    x：数据集\n",
    "    ｉ：第ｉ个特征\n",
    "    \"\"\"\n",
    "    if x.ndim == 1:\n",
    "        x = x.reshape((len(x),1)) # 如果是一维的，那么定义为(len(x)行1列)的数据分布\n",
    "    col_i_data = x[:,i] # 第ｉ列数据\n",
    "    uniquevalue = np.unique(col_i_data) # 该属性下查找所有可能的值\n",
    "    H_X = 0 # 统计熵的加和\n",
    "    for j in range(len(uniquevalue)): # \n",
    "        i_p = Counter(col_i_data)[j+1]/len(col_i_data) # 对每个属性值计数并计算概率值（占比）\n",
    "        if i_p != 0:\n",
    "            i_logp = np.log2(i_p) # 对数值\n",
    "            H_X = H_X + (-i_p*i_logp) # 累加\n",
    "        elif i_p == 0:\n",
    "            H_X = H_X + 1 # 对于概率值为0的，定义0*log0=1，并累加\n",
    "    return H_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（2）**经验熵**的定义：\n",
    ">实际是对$y$求熵，即$H(Y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "经验熵 = 0.970950594455\n"
     ]
    }
   ],
   "source": [
    "print('经验熵 =',cal_entropy(y,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（3）**\"相对熵\"**的定义：\n",
    ">$$H(Y|X) = \\sum_{i=1}^n H(Y|X=x_i)$$\n",
    "表示在已知随机变量$X$的条件下随机变量$Y$的不确定性．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_mutualinformation(x,y,i):\n",
    "    \"\"\"\n",
    "    计算第ｉ列的条件熵\n",
    "        x: 数据\n",
    "        y: 类别标签\n",
    "        i: 第ｉ列\n",
    "    \"\"\"\n",
    "    col_i_data = x[:,i]\n",
    "    data = np.column_stack((col_i_data,y)) # 合并了一个数组，[ｘ_i,y]\n",
    "    uniquevalue = np.unique(col_i_data) # 查找所有可能的值\n",
    "    H_YconX = 0 # 统计熵的加和\n",
    "    H_YconX = 0\n",
    "    for j in range(len(uniquevalue)):\n",
    "        D_i = data[col_i_data == uniquevalue[j],:]\n",
    "        uniqueclass = np.unique(y)\n",
    "        factor2 = 0\n",
    "        for k in range(len(uniqueclass)):# H(Di) = sum(|Dik|/Di * log |Dik|/Di)\n",
    "            D_ik = D_i[D_i[:,1] == uniqueclass[k],:]\n",
    "            if (D_ik.shape[0] == 0):\n",
    "                temp2 = 0\n",
    "            else:\n",
    "                temp2 = - D_ik.shape[0]/D_i.shape[0] * np.log2(D_ik.shape[0]/D_i.shape[0])\n",
    "                # print(temp2)\n",
    "            factor2 = factor2 + temp2\n",
    "        H_YconX = H_YconX + len(D_i)/len(col_i_data) * factor2\n",
    "    return H_YconX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2信息增益"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "信息增益为\n",
    ">$$g(D,A) = H(D)-H(D|A)$$\n",
    "表示$A$特征对训练数据$D$的信息增益"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 0 个特征的信息增益值为： 0.0830074998558\n",
      "第 1 个特征的信息增益值为： 0.323650198152\n",
      "第 2 个特征的信息增益值为： 0.419973094022\n",
      "第 3 个特征的信息增益值为： 0.362989562537\n"
     ]
    }
   ],
   "source": [
    "for m in range(x.shape[1]):\n",
    "    print('第',m,'个特征的信息增益值为：',cal_entropy(y,0) - cal_mutualinformation(x,y,m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">将上面的过程定义成函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.083007499855768829,\n",
       " 0.32365019815155627,\n",
       " 0.41997309402197491,\n",
       " 0.36298956253708536]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cal_information_gain(x,y):\n",
    "    \"\"\"\n",
    "    计算每个特征下的信息增益值\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for i in range(x.shape[1]):\n",
    "        value = cal_entropy(y,0) - cal_mutualinformation(x,y,i)\n",
    "        result.append(value)\n",
    "    return result\n",
    "cal_information_gain(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">所以上面选择增益最大的(0.419973)，即第２个特征（有无房子）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5.3 定义自己的决策树函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDecisionTree():\n",
    "    def train():\n",
    "        \"\"\"\n",
    "        训练\n",
    "        \"\"\"    \n",
    "    \n",
    "    def predict():\n",
    "        \"\"\"\n",
    "        测试\n",
    "        \"\"\"    \n",
    "    \n",
    "    def cal_entropy():\n",
    "        \"\"\"\n",
    "        计算熵\n",
    "        \"\"\"    \n",
    "    \n",
    "    def cal_mutualinformation():\n",
    "        \"\"\"\n",
    "        计算互信息值\n",
    "        \"\"\"\n",
    "    \n",
    "    def cal_information_gain():\n",
    "        \"\"\"\n",
    "        \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
